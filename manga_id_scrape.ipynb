{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time \n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grab_ids(limit_param):\n",
    "    '''\n",
    "    This function scrapes a page of the top mangas list on MyAnimeList for the manga ids. Each page will provide us\n",
    "    with 50 unique ids of manga series.\n",
    "\n",
    "    Parameters:\n",
    "        limit_param: (int) Determines the page we will scrape (rankings limit_param + 1 to limit_param + 50, inclusive)\n",
    "    \n",
    "    Outputs:\n",
    "        temp_manga_ids: (set) Holds the manga id for the series on this page (use the manga id to access https://myanimelist.net/manga/<manga_id>)\n",
    "    '''\n",
    "\n",
    "    temp_manga_ids = []\n",
    "\n",
    "    data = requests.get(f'https://myanimelist.net/topmanga.php?limit={limit_param}')\n",
    "    if data.status_code != 200:\n",
    "        print(f'Failed to scrape web page with status code: {data.status_code}')\n",
    "        return \n",
    "    \n",
    "    soup = BeautifulSoup(data.text, 'html.parser')\n",
    "    rows = soup.find_all('tr', {'class': 'ranking-list'}) # Represents each of 50 rows for the 50 series on the page\n",
    "\n",
    "    for row in rows:\n",
    "        # Extracts the manga_id from https://myanimelist.net/manga/<manga_id>/<manga_name>\n",
    "        manga_id = row.find('a', {'class': 'fw-b'}).get('href').split('/')[-2] \n",
    "\n",
    "        temp_manga_ids.append(manga_id)\n",
    "\n",
    "    return temp_manga_ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SERIES_NUMBER = 16601 # Represents the page with the last series that have a rating (rest have N/A)\n",
    "manga_ids = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, MAX_SERIES_NUMBER, 50):\n",
    "    manga_ids.extend(grab_ids(i)) # Maintain a set of all manga ids that are scraped\n",
    "    time.sleep(5) # Make sure to not overflow the MyAnimeList server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only use the series from 0-16612 since anything after doesn't have a rating\n",
    "df = pd.DataFrame(manga_ids[:16613], columns = ['manga_id'])\n",
    "df.to_csv('manga_ids.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('manga_env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5e3ec886060125e71f64d2f8f5e7dcae200771887f0af3243723a1d274ad282f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
